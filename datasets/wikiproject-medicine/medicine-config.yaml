## Configuration file for creating a snapshot of WikiProject China
name: "WikiProject Medicine"

## Full category name (without namespace) for each of the four
## importance categories we are concerned with:
importance categories: {
    Top: "Top-importance medicine articles",
    High: "High-importance medicine articles",
    Mid: "Mid-importance medicine articles",
    Low: "Low-importance medicine articles"
    }

## Additional categories of articles, these are used when counting
## inlinks from within the project.
support categories:
 - "NA-importance medicine articles"
 - "Unknown-importance medicine articles"

## Path to the snapshot dataset TSV for this project.
snapshot file: wikiproject-medicine-snapshot-20170619.tsv

## Path to the TSV containing articles identified as disambiguation pages
disambiguation file: wikiproject-medicine-disambiguations-20170619.tsv

## Path to the TSV dataset with inlink counts, pageviews, and Wikidata items
dataset: wikiproject-medicine-dataset-20170619.tsv

## Path to the TSV clickstream dataset with clicks, referrer counts, etc
clickstream file: wikiproject-medicine-clickstream-20170619.tsv

## Path to the GEXF file with the Wikidata network
wikidata network: wikiproject-medicine-network-20170619.gexf

## Path to the wikitable with articles that might need rerating
prediction table: wikiproject-medicine-predictions-20170619.txt

## Predictive columns
predictors:
 - "rank_views_perc"
 - "rank_links_perc"
 - "prop_proj_inlinks"
 - "prop_from_art"
 - "prop_act_inlinks"

## Label column
labels: "importance_rating"

## Number of articles to use for training and test sets when training
## and evaluating the model:
test set size: 40
training set size: 50

## Configuration for SMOTE, which class should be SMOTEd, and how many
## times larger the SMOTEd training set should be:
SMOTE evaluation: 1
SMOTE class: "Top"
SMOTE factor: 9

## Number of articles to use for training the final model, and whether
## to use SMOTE for oversampling there as well (with same settings as above):
final training size: 90
SMOTE final: 1

## Model parameters, ref http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html
model parameters: {
    'n_estimators' : 500,
    'learning_rate': 0.05,
    'max_depth': 7,
    'min_samples_leaf': 32,
    'random_state': 42
}

## Where to write out the trained model
model file: wikiproject-nfl.gbm.model
